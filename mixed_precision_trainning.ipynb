{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e87f1fbf",
   "metadata": {},
   "source": [
    "### 2.3.7  Mixed-Precision Training (Megatron-style)\n",
    "\n",
    "---\n",
    "\n",
    "#### Why mix precisions at all?\n",
    "\n",
    "1. **Memory & speed**\n",
    "\n",
    "   * Model weights, activations, and gradients are stored in **fp16** (half-precision) to cut memory and bandwidth in half.\n",
    "   * A second copy of the weights is kept in **fp32** (“master weights”) so that the **Adam** update remains numerically stable.\n",
    "\n",
    "2. **Gradient underflow protection**\n",
    "\n",
    "   * Roughly two-thirds of language-model gradients have magnitudes < 2⁻²⁴ (the smallest positive fp16), so a pure-fp16 run would zero-out many updates and diverge.\n",
    "   * Mixed precision keeps the math in range: forward/backward in fp16, critical updates in fp32.\n",
    "\n",
    "---\n",
    "\n",
    "#### End-to-end workflow\n",
    "\n",
    "| Step                       | What happens                                                                                                   | Precision               |\n",
    "| -------------------------- | -------------------------------------------------------------------------------------------------------------- | ----------------------- |\n",
    "| **1. Copy & cast**         | At launch, duplicate each fp32 parameter into fp16. Adam’s **momentum** & **variance** states remain fp32.     | fp32 → fp16 copy        |\n",
    "| **2. Forward pass (FWD)**  | Run the model with **fp16 weights & activations**.                                                             | fp16                    |\n",
    "| **3. Loss scaling**        | Multiply the fp32 loss by $2^{\\text{loss\\_scale}}$ to avoid tiny gradients.                                    | fp32                    |\n",
    "| **4. Backward pass (BWD)** | Compute **scaled gradients**; store them in fp16 to save memory.                                               | fp16                    |\n",
    "| **5. Unscale gradients**   | Convert to fp32 and divide by $2^{\\text{loss\\_scale}}$.                                                        | fp32                    |\n",
    "| **6. Clip / regularise**   | Apply gradient-norm clipping, weight-decay, etc., on the fp32 gradients.                                       | fp32                    |\n",
    "| **7. Parameter update**    | `optimizer.step()` updates **master fp32 weights**, then casts the result back to fp16 for the next iteration. | fp32 update → fp16 copy |\n",
    "\n",
    "> **Diagram:** The attached flowchart (green = fp32, purple = fp16) follows exactly these seven stages.\n",
    "\n",
    "---\n",
    "\n",
    "#### Memory breakdown (per parameter “ϕ”)\n",
    "\n",
    "| Category                   | Precision           | Footprint |\n",
    "| -------------------------- | ------------------- | --------- |\n",
    "| **Must-have**              | master param (fp32) | 4 ϕ       |\n",
    "|                            | momentum (fp32)     | 4 ϕ       |\n",
    "|                            | variance (fp32)     | 4 ϕ       |\n",
    "| **Transient**              | param copy (fp16)   | 2 ϕ       |\n",
    "|                            | gradients (fp16)    | 2 ϕ       |\n",
    "| **Total (no activations)** |                     | **16 ϕ**  |\n",
    "\n",
    "*If you cast the fp16 weights on-the-fly instead of storing them persistently, the extra 2 ϕ vanish, but most frameworks keep the copy for speed.*\n",
    "\n",
    "---\n",
    "\n",
    "#### Loss-scale mechanics\n",
    "\n",
    "1. **Static loss scale (cheap but brittle)**\n",
    "\n",
    "   * Pick a constant power-of-2 scale (e.g. 2¹⁶).\n",
    "   * *Scale-up*: multiply the loss before BWD.\n",
    "   * *Scale-down*: divide gradients after BWD.\n",
    "   * If any **inf / NaN** appears, skip the update for that step.\n",
    "\n",
    "2. **Dynamic loss scale (AMP default)**\n",
    "\n",
    "   * Begin with a large scale (e.g. 2²⁴).\n",
    "   * After each step, check for **inf / NaN** in the fp16 grads.\n",
    "   * * If clean → optionally **increase** the scale every *N* steps.\n",
    "   * * If overflow → **halve** the scale and redo the step with the previous weights.\n",
    "\n",
    "This keeps gradients as large as possible without blowing up.\n",
    "\n",
    "---\n",
    "\n",
    "#### Gradient clipping\n",
    "\n",
    "Use L2-norm clipping on the **unscaled fp32 gradients**:\n",
    "\n",
    "$$\n",
    "g_1 = \\frac{\\partial J}{\\partial w_1},\\;\n",
    "g_2 = \\frac{\\partial J}{\\partial w_2},\\;\n",
    "\\|g\\|_2 = \\sqrt{g_1^2 + g_2^2}.\n",
    "$$\n",
    "\n",
    "If $\\|g\\|_2 > c$ (threshold), rescale: $g \\leftarrow \\tfrac{c}{\\|g\\|_2} \\, g$.\n",
    "\n",
    "---\n",
    "\n",
    "#### Key take-aways\n",
    "\n",
    "* **One copy, two precisions:** fp16 for speed, fp32 for accuracy.\n",
    "* **Loss scaling is mandatory** to stop gradients from under-flowing.\n",
    "* **Dynamic scaling + gradient clipping** makes training far more crash-resistant.\n",
    "* Memory overhead is modest—about **1.5×** pure-fp16—yet avoids the accuracy pitfalls of full half-precision training.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
