{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0c36f2ae",
   "metadata": {},
   "source": [
    "**2.3.2 Determining Model Architecture & Memory Budget**\n",
    "\n",
    "* **Architecture**\n",
    "  All of our variants use the “LLaMA-style” backbone (RoPE positional embeddings + grouped-query attention + RMSNorm + SwiGLU activations).\n",
    "\n",
    "* **Memory Footprint by Precision & Fine-tuning Method**\n",
    "  \\| Method                             | Bits │ 7 B    │ 14 B   │ 30 B    │ 70 B     │ ≈× B   │\n",
    "  \\|------------------------------------|:----:|:------:|:------:|:-------:|:--------:|:------:|\n",
    "  \\| **Full (fp32)**                    |  32  | 120 GB | 240 GB | 600 GB  | 1 200 GB | \\~18× GB|\n",
    "  \\| **Full (pure bf16)**               |  16  |  60 GB | 120 GB | 300 GB  |  600 GB  | \\~8× GB |\n",
    "  \\| Freeze/LoRA/GaLoRe/APOLLO/BAdam\\*  |  16  |  16 GB |  32 GB |  64 GB  |  160 GB  | \\~2× GB |\n",
    "  \\| **QLoRA (8-bit)**                  |   8  |  10 GB |  20 GB |  40 GB  |   80 GB  | \\~× GB  |\n",
    "  \\| **QLoRA (4-bit)**                  |   4  |   6 GB |  12 GB |  24 GB  |   48 GB  | \\~×/2 GB|\n",
    "  \\| **QLoRA (2-bit)**                  |   2  |   4 GB |   8 GB |  16 GB  |   24 GB  | \\~×/4 GB|\n",
    "\n",
    "  > **Key takeaway:**\n",
    "  >\n",
    "  > * **Full-precision** training (fp32/bf16) demands hundreds of GB for multi-billion-parameter models.\n",
    "  > * **Freeze-and-LoRA-style** methods halve that requirement.\n",
    "  > * **QLoRA** in 4- or 2-bit mode slashes GPU memory further, making even 30 B+ models feasible on commodity hardware.\n",
    "\n",
    "---\n",
    "\n",
    "**2.3.3 Choosing a Training Framework**\n",
    "\n",
    "* **Recommended:**\n",
    "\n",
    "  * **Megatron-LM** for pure pre-training.\n",
    "  * If you’re working with the Qwen family, grab **Alibaba’s Pai-Megatron-Patch**:\n",
    "    [https://github.com/alibaba/Pai-Megatron-Patch](https://github.com/alibaba/Pai-Megatron-Patch)\n",
    "\n",
    "* **Avoid** using DeepSpeed (and its OpenRLHF/DeepSpeed-Chat wrappers) for your *pre-training* stage.\n",
    "\n",
    "> **Why Megatron-LM?**\n",
    ">\n",
    "> 1. **Optimized parallelism** – Megatron’s tensor- and pipeline-parallel kernels are highly tuned (RoPE lives in APEX ops, and an APEX MLP kernel is on the way).\n",
    "> 2. **Transparent config** – `arguments.py` exposes hundreds of knobs (dropout, layer settings, etc.), so you can dial in exactly what you need.\n",
    "> 3. **Fast startup** – Even a 100 B-parameter model loads in under a minute, which makes iterative debugging a breeze.\n",
    "\n",
    "---\n",
    "\n",
    "**2.3.4 Pre-training Strategy**\n",
    "\n",
    "> **Study these reference recipes:**\n",
    ">\n",
    "> * **MiniCPM** series\n",
    "> * **phi** series\n",
    "> * **DeepSeekMath**\n",
    "\n",
    "They provide battle-tested schedules, hyperparameters, and curriculum techniques that you can adapt for your own large-scale Chinese pre-training.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
