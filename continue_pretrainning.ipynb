{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "48529b2b",
   "metadata": {},
   "source": [
    "### 2.3.8 Continued Pretraining\n",
    "\n",
    "---\n",
    "\n",
    "#### **Domain Continued Pretraining (DCTP)**\n",
    "\n",
    "* **Purpose:**\n",
    "  After base pretraining, further pretrain on domain-specific data to inject specialized knowledge (e.g. code models like CodeLlama).\n",
    "\n",
    "* **How:**\n",
    "  Same pretraining pipeline (no architectural change), but using domain-specific corpora.\n",
    "\n",
    "* **Data mixing:**\n",
    "\n",
    "  * Usually mix general-domain and domain-specific data.\n",
    "  * Suggested mixing ratio: **7 : 2 : 1** â†’\n",
    "    70% general base data,\n",
    "    20% domain-specific data,\n",
    "    10% instruction or high-value data.\n",
    "\n",
    "* **Typical pipeline:**\n",
    "\n",
    "  * Continue training using the same tokenizer & model.\n",
    "  * Clean + deduplicate domain data beforehand.\n",
    "\n",
    "---\n",
    "\n",
    "#### **Long Context Continued Pretraining (LCTP)**\n",
    "\n",
    "* **Reference Papers:**\n",
    "\n",
    "  * CodeLlama\n",
    "  * Effective Long-Context Scaling of Foundation Models\n",
    "  * YaRN: Efficient Context Window Extension of Large Language Models\n",
    "\n",
    "* **Setup:**\n",
    "\n",
    "  * Use roughly **20B tokens** of long-text data.\n",
    "  * Use **CodeLlama-style** long-context training.\n",
    "  * Prefer **NTK-Aware** scaling (adjust RoPE frequencies).\n",
    "\n",
    "* **Hyperparameters:**\n",
    "  Based on CodeLlamaâ€™s long-context finetuning setup:\n",
    "\n",
    "  * Gradually increase context length:\n",
    "\n",
    "    * Start from 4k tokens â†’ grow to 16k tokens.\n",
    "  * Modify RoPE scaling factor:\n",
    "\n",
    "    $$\n",
    "    \\theta = \\left(\\frac{8}{2\\sqrt{d}}\\right)\n",
    "    $$\n",
    "\n",
    "    * Example: if hidden size $d=100000$, $\\theta \\approx 1000000$.\n",
    "  * This scaling helps reduce attention instability for distant tokens.\n",
    "  * Smoothly adapting to longer contexts helps models generalize across different sequence lengths.\n",
    "\n",
    "* **Engineering Notes:**\n",
    "\n",
    "  * Add **context-parallel attention** to speed up training on long sequences.\n",
    "  * Carefully redesign attention (flash attention, high-efficiency attention kernels).\n",
    "  * Expand positional embeddings up to **32k / 128k tokens** (gradual increase starting from \\~8k).\n",
    "\n",
    "* **Common Long Context Use Cases:**\n",
    "\n",
    "  * Document-level reasoning\n",
    "  * Code generation\n",
    "  * Dialogue models with very long turns\n",
    "\n",
    "---\n",
    "\n",
    "#### **Key Idea Summary**\n",
    "\n",
    "* Domain continued pretraining injects *what* the model should know.\n",
    "* Long context continued pretraining teaches *how* the model handles longer sequences.\n",
    "* Both are often combined in high-end models like **CodeLlama 34B, GPT-4o, Claude 3, Gemini** etc.\n",
    "\n",
    "---\n",
    "\n",
    "ðŸ‘‰ If you want, I can also give you **practical recipes** for how to implement these two kinds of continued pretraining â€” very useful for real-world finetuning.\n",
    "Shall we?\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
