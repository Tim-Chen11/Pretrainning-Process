{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2be30b7",
   "metadata": {},
   "source": [
    "**2.3.3 Training-Framework Selection**\n",
    "\n",
    "> **Recommendation:**\n",
    "> • For large-scale pre-training, use **Megatron-LM**.\n",
    "> • If you’re training a Qwen model, grab Alibaba’s **Pai-Megatron-Patch** ([https://github.com/alibaba/Pai-Megatron-Patch](https://github.com/alibaba/Pai-Megatron-Patch)).\n",
    "> • **Do not** use DeepSpeed (including OpenRLHF or DeepSpeed-Chat) for the *pre-training* phase.\n",
    "\n",
    "**Why Megatron-LM?**\n",
    "\n",
    "1. **Blazing-fast parallelism**\n",
    "   – Its tensor-parallel and pipeline-parallel kernels are highly optimized (RoPE lives in APEX ops, and an APEX MLP kernel is coming soon).\n",
    "2. **Full visibility into hyperparameters**\n",
    "   – The `arguments.py` config file exposes hundreds of knobs (dropout rates, layer-specific settings, etc.), giving you fine-grained control.\n",
    "3. **Rapid startup & debugging**\n",
    "   – Even a 100 B-parameter model loads in under a minute, so iterative testing and debugging are very convenient.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
