{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6b813c35",
   "metadata": {},
   "source": [
    "**2.3.4 Pre-training Strategy**\n",
    "\n",
    "> **Reference recipes:**\n",
    "> • MiniCPM series\n",
    "> • phi series\n",
    "> • DeepSeekMath\n",
    "\n",
    "These give battle-tested schedules, hyperparameters, and curriculum ideas you can adapt for large-scale Chinese pre-training.\n",
    "\n",
    "---\n",
    "\n",
    "### Optimal Batch Size\n",
    "\n",
    "* **Why it matters:**\n",
    "  The batch size trades off convergence speed against compute & memory cost.\n",
    "\n",
    "  * If it’s **too large**, each step consumes huge amounts of data and compute, and beyond a point you see diminishing returns on loss reduction.\n",
    "  * If it’s **too small**, you waste wall-clock time taking many more steps—and you may not reduce the loss efficiently.\n",
    "\n",
    "* **Empirical study:**\n",
    "  On models of 0.009 B, 0.036 B, and 0.17 B parameters, six different batch sizes were tested. The loss surface (C4 loss) vs. batch size and training steps shows a clear “ridge” of optimum batch-size for each model scale.\n",
    "\n",
    "* **Rule of thumb for C4 loss:**\n",
    "\n",
    "  $$\n",
    "    \\text{Optimal batch size} \\;\\;=\\;\\; \\frac{1.2110 \\times 10^9}{L^{6.2393}}\n",
    "  $$\n",
    "\n",
    "  where $L$ is the sequence length.\n",
    "\n",
    "*(In the published plots, the red curve traces this optimum across models.)*\n",
    "\n",
    "---\n",
    "\n",
    "### WSD (Warmup–Steady–Decay) Scheduler\n",
    "\n",
    "Most large-model pre-training naturally falls into three phases:\n",
    "\n",
    "1. **Warmup**\n",
    "2. **Steady-state**\n",
    "3. **Decay (annealing)**\n",
    "\n",
    "During decay, you typically introduce higher-quality data and apply an annealing schedule (e.g. cosine). The **WSD scheduler** formalizes this:\n",
    "\n",
    "* Let\n",
    "\n",
    "  * $W$ = number of steps in warmup,\n",
    "  * $S$ = end of steady-state,\n",
    "  * $D$ = length of decay.\n",
    "\n",
    "* Define the learning rate at step $s$ as:\n",
    "\n",
    "  $$\n",
    "    \\ell r(s) =\n",
    "    \\begin{cases}\n",
    "      \\displaystyle \\frac{s}{W}\\,\\eta, & 0 \\le s < W, \\\\[6pt]\n",
    "      \\eta,                         & W \\le s < S, \\\\[3pt]\n",
    "      f(s - S)\\,\\eta,               & S \\le s < S + D,\n",
    "    \\end{cases}\n",
    "  $$\n",
    "\n",
    "  where\n",
    "\n",
    "  * $\\eta$ = maximum learning rate,\n",
    "  * $f(\\cdot)$ is any monotonically decreasing function with $0 < f(\\cdot)\\le1$.\n",
    "\n",
    "**Benefits of WSD:**\n",
    "\n",
    "1. You can **pause or snapshot** at any phase.\n",
    "2. It often **outperforms** a pure cosine schedule.\n",
    "3. Phases are **explicit**, making it easy to swap in different data or curricula.\n",
    "\n",
    "---\n",
    "\n",
    "### Pre-training Tricks\n",
    "\n",
    "* **Keep it simple for long runs:**\n",
    "  CPT-style runs can last **>1 month**. If your GPU memory suffices, **avoid** adding extra parallelism (tensor\\_, pipeline\\_, sequence\\_parallel), offload, or gradient-recompute—these only complicate debugging.\n",
    "\n",
    "* **Include instruction data:**\n",
    "  Early evidence suggests that **mixing in instruction-style examples** (i.e.\\ “prompt→response” pairs) helps your model learn more versatile generation behavior.\n",
    "\n",
    "---\n",
    "\n",
    "### Multi-Stage Training Flow\n",
    "\n",
    "A typical four-stage workflow once your data and code are ready:\n",
    "\n",
    "1. **Warmup**\n",
    "   Slowly ramp your LR up to the maximum.\n",
    "\n",
    "2. **Main training**\n",
    "   Use a schedule like cosine, cosine-decay, constant, or constant-decay. Decide via small-model experiments or literature.\n",
    "\n",
    "3. **Long-context adaptation**\n",
    "   Increase RoPE’s base frequency and bump up sequence length so the model masters longer texts.\n",
    "\n",
    "4. **Final annealing**\n",
    "   Train on high-quality or in-domain (IFT) data to “hone” the model for benchmarks and downstream tasks.\n",
    "\n",
    "> **In practice** you often run **two or more phases**:\n",
    ">\n",
    "> 1. First, train on the **entire corpus**.\n",
    "> 2. Then, fine-tune on a **smaller slice**.\n",
    "> 3. Finally, do a **short anneal** on a **high-quality subset**.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
