{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d68c4cac",
   "metadata": {},
   "source": [
    "**2.3.6 Pre-training Scaling Laws**\n",
    "\n",
    "---\n",
    "\n",
    "### Estimating Your FLOPs Budget\n",
    "\n",
    "Before you start pre-training, you can calculate how much compute you’ll need using:\n",
    "\n",
    "$$\n",
    "\\text{FLOPs} \\;=\\; 6 \\;\\times\\; (\\#\\text{tokens}) \\;\\times\\; (\\#\\text{parameters})\n",
    "$$\n",
    "\n",
    "* The factor of 6 accounts for forward+backward passes.\n",
    "* You can convert this into “GPU × days.”\n",
    "\n",
    "  * **Example:** 100 × NVIDIA A800 GPUs running for 30 days.\n",
    "\n",
    "    * One A800 sustains ∼210 TFLOPs/s.\n",
    "    * Total FLOPs ≈ $210\\!\\times\\!10^{12}$ FLOPs/s × 100 GPUs × 30 days × 24 h × 3600 s ≈ $5.4\\times10^{22}$ FLOPs.\n",
    "\n",
    "**Common Units**\n",
    "\n",
    "* 1 T-token = $10^{12}$ tokens\n",
    "* 1 B-parameter = $10^9$ parameters\n",
    "\n",
    "---\n",
    "\n",
    "### Compute-Optimal Model Sizing\n",
    "\n",
    "Given a fixed FLOPs budget,\n",
    "\n",
    "$$\n",
    "(\\#\\text{tokens}) \\;\\times\\; (\\#\\text{parameters}) = \\text{constant.}\n",
    "$$\n",
    "\n",
    "* **Trade-off:** More params ↔ fewer tokens; fewer params ↔ more tokens.\n",
    "* **Illustration (for $5.4\\times10^{22}$ FLOPs):**\n",
    "\n",
    "  * A 7 B-param model → ≈ 10 T tokens\n",
    "  * A 70 B-param model → ≈ 1 T tokens\n",
    "\n",
    "---\n",
    "\n",
    "### LLaMA3’s Empirical Scaling Law\n",
    "\n",
    "LLaMA3’s authors ran experiments over budgets $6\\times10^{18}$ to $10^{22}$ FLOPs and model sizes from 40 M to 16 B params, measuring validation loss at each “iso-FLOPs” slice. They then fit a power law for the **compute-optimal token count** $N^*(C)$ at budget $C$:\n",
    "\n",
    "$$\n",
    "N^*(C) = A\\,C^\\alpha\n",
    "\\quad\\text{with fitted parameters }\n",
    "\\alpha = 0.537,\\;\n",
    "A = 0.299.\n",
    "$$\n",
    "\n",
    "Plotting these predicted values against the measured optima shows excellent agreement across four orders of magnitude.\n",
    "\n",
    "---\n",
    "\n",
    "### Predicting Downstream Metrics (e.g. ARC-Challenge)\n",
    "\n",
    "1. **Benchmark setup:**\n",
    "\n",
    "   * For each multiple-choice option, compute the model’s perplexity (NLL) and select the lowest-PPL choice.\n",
    "2. **Two-step prediction pipeline:**\n",
    "\n",
    "   * **Step 1:** Use the scaling-law fit ($N^*(C)$ vs. FLOPs) and the small-model NLL-vs-FLOPs data to predict the model’s NLL on the ARC validation set before you train.\n",
    "   * **Step 2:** Fit a sigmoid mapping from normalized NLL → accuracy (using data from smaller models and LLaMA 2).\n",
    "3. **Outcome:** This lets you forecast a model’s final accuracy *before* spending huge compute. On 405 B params, the method under-estimated accuracy by only a hair.\n",
    "\n",
    "---\n",
    "\n",
    "### Practical Takeaways\n",
    "\n",
    "* For models **≤ 70 B**, you’re often constrained by parameter count (model size) more than compute budget—so “compute-optimal” sizing matters less.\n",
    "* At **larger scales**, where FLOPs are capped, finding the compute-optimal trade-off is crucial.\n",
    "* As future hardware makes 400 B+ models relatively “small,” you’ll be able to allocate even more tokens.\n",
    "* **Scaling laws** not only identify the best size but also quantify your return on investment (compute cost, carbon footprint, etc.), guiding cost-benefit analyses and dev-loss projections.\n",
    "* While this approach works very well for multiple-choice tasks, complex reasoning (chain-of-thought, math) may require more sophisticated predictive frameworks.\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
